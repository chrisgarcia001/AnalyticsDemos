library(caret)
source('classFunctions.R')
source('exploratoryFunctions.R')
source('interpretationFunctions.R')

# ------------------------------------------------------------------------------------------------
# This is for initial exploratory analysis to see how high- and low-star ratings compare in terms
# of frequent words. 
# ------------------------------------------------------------------------------------------------

#dstmp <- read.csv('./yelp_data/review_data_100k.csv')
dstmp <- read.csv('./yelp_data/review_data_15k.csv')

build.wordcloud(dstmp)
#stars.compare.wordplot(dstmp)  # Example - compare all star levels
stars.compare.wordplot(dstmp, layout=c(1,3), max.percentile=0.98)

# --------- # For Demo: Compare 1- and 5-star levels, no PNG
stars.compare.wordplot(dstmp, use.stars=c(1, 5), layout=c(1, 2), max.percentile=0.85) 
# ---------

stars.compare.wordplot(dstmp, use.stars=c(1, 5), layout=c(1, 2), max.percentile=0.85, save.as.png="images/stars-1-5.png")
stars.compare.wordplot(dstmp, use.stars=1, layout=c(1, 1), max.percentile=0.85, save.as.png="images/stars-1.png")
stars.compare.wordplot(dstmp, use.stars=5, layout=c(1, 1), max.percentile=0.85, save.as.png="images/stars-5.png")

# ------------------------------------------------------------------------------------------------
# This is the main model-building section. After this is run the R workspace can be saved
# as a snapshot so that variable stack.predictf contains the prediction model and can be 
# applied to new datasets. 
# ------------------------------------------------------------------------------------------------

message('Building Dataset...')
curr.data <- balanced.traintest.data("./yelp_data/review_data_100k.csv", 25000, trainp=0.7, sparsity.cutoff = 0.93) 
#curr.data <- balanced.traintest.data("./yelp_data/review_data_15k.csv", 5000, trainp=0.7, sparsity.cutoff = 0.93) # Smaller test example
message('Building Model Stack...')
inSub <- createDataPartition(y = curr.data$training$target.stars, p = 0.6, list = FALSE)
train.sub <- curr.data$training[inSub,]
train.top <- curr.data$training[-inSub,]
stack.predictf <- model.stack.predictorf(train.sub, train.top, top.model="rf", sub.models=c("qda","gbm","rf"))
head(stack.predictf(curr.data$testing))
print(confusionMatrix(stack.predictf(curr.data$testing), curr.data$testing$target.stars))

# ------------------------------------------------------------------------------------------------
# This is for the model performance analysis. If the above section is not run, this part assumes the 
# workspace generated by the above section
# ------------------------------------------------------------------------------------------------

head(build.summary.df(curr.data$testing$target.stars, stack.predictf(curr.data$testing)))
plot.results(curr.data$testing$target.stars, stack.predictf(curr.data$testing), 
			 col="yellow")#, save.as.png="images/results.png")

raw.data <- read.csv('./yelp_data/review_data_100k.csv')			 
sample.size <- 500
sample.pred.data <- curr.data$testing[sample(1:nrow(curr.data$testing), sample.size),]
sample.pred.data$actual <- as.numeric(sample.pred.data$target.stars)
sample.data <- raw.data[rownames(sample.pred.data),]
sample.data$actual <- sample.pred.data$actual
sample.data$predicted <- as.numeric(stack.predictf(sample.pred.data))
#head(stack.predictf(sample.pred.data))
write.csv(sample.data[,c("text", "actual", "predicted")], "./output_data/sample_predictions.csv", row.names=FALSE)

edit(read.csv("./output_data/sample_predictions.csv"))





			 

